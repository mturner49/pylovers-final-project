{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pip\n",
    "# pip.main(['install', '{insert_pckg_here}'])\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "#Exploratory Data Analysis\n",
    "from statsmodels.stats.outliers_influence import variance_inflation_factor\n",
    "from statsmodels.tools.tools import add_constant"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading Training/Testing Data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_url = 'https://raw.githubusercontent.com/mturner49/pylovers-final-project/dev/data/train.csv'\n",
    "test_url = 'https://raw.githubusercontent.com/mturner49/pylovers-final-project/dev/data/test.csv'\n",
    "\n",
    "train_df = pd.read_csv(train_url, low_memory=False, error_bad_lines=False, index_col='Id')\n",
    "test_df = pd.read_csv(test_url, low_memory=False, error_bad_lines=False, index_col='Id')\n",
    "\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is for checking datatypes\n",
    "train_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preprocessing:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df that consists of columns and the number of missing values for each \n",
    "data = train_df.isnull().sum().sort_values(ascending=False)\n",
    "missing_df = pd.DataFrame(data=data, columns=['missing_cnt'])\n",
    "\n",
    "# add column and fill it with the percentage of those missing values\n",
    "missing_df['percent_missing'] = missing_df.missing_cnt.apply(lambda x : '{:.2f}'.format(x/train_df.shape[0] * 100)) \n",
    "missing_df = missing_df[missing_df.missing_cnt > 0]\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns where majority of their values are missing\n",
    "train_df = train_df.drop(['PoolQC', 'MiscFeature', 'Fence', 'FireplaceQu'], axis = 1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vida\n",
    "# I think we should drop 'Alley' as well! with Just 91 non-null values, like 93.77% null!\n",
    "train_df = train_df.drop(['Alley'], axis = 1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 'GarageType','GarageYrBlt','GarageFinish','GarageCars','GarageQual','GarageCond' columns has 1379 not null values!\n",
    "# So, I tested the theory that exactly same rows has null for these columns!\n",
    "# All 81 do not have Garage! So, we can put zero instead!\n",
    "train_df.loc[:,['GarageCars','GarageArea','GarageType','GarageYrBlt','GarageFinish','GarageCars','GarageQual','GarageCond']][train_df['GarageType'].isnull()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 81 do not have Garage! So, we can put zero instead!\n",
    "train_df.update(train_df[['GarageType','GarageYrBlt','GarageFinish','GarageCars','GarageQual','GarageCond']].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for Basement: 'BsmtFinType2','BsmtExposure','BsmtQual','BsmtCond','BsmtFinType1'\n",
    "train_df.loc[:,['TotalBsmtSF','BsmtUnfSF','BsmtFinType2','BsmtExposure','BsmtQual','BsmtCond','BsmtFinType1']][train_df['BsmtCond'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# All 37 do not have Garage! So, we can put zero instead!\n",
    "train_df.update(train_df[['BsmtQual','BsmtCond','BsmtFinType1']].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same for Basement: 'BsmtFinType2','BsmtExposure'\n",
    "train_df.loc[:,['TotalBsmtSF','BsmtUnfSF','BsmtFinType2','BsmtExposure','BsmtQual','BsmtCond','BsmtFinType1']][train_df['BsmtExposure'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.at[949,'BsmtExposure']='No'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.loc[:,['TotalBsmtSF','BsmtUnfSF','BsmtFinSF2','BsmtFinType2','BsmtExposure','BsmtQual','BsmtCond','BsmtFinType1']][train_df['BsmtFinType2'].isnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# I'm going with 'Average Living Quarters'\n",
    "train_df.at[333,'BsmtFinType2']='ALQ'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.update(train_df[['BsmtFinType2','BsmtExposure']].fillna(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vida: let's keep the dummies for after Exploratory Data Analysis!\n",
    "# train_df1 = pd.get_dummies(data=train_df , columns=['MSZoning','Street', 'Alley', 'LotShape', 'LandContour',\n",
    "#                                                    'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood',\n",
    "#                                                    'BldgType', 'HouseStyle'])\n",
    "# train_df1 = pd.get_dummies(data=train_df , columns=['MSZoning','Street', 'LotShape', 'LandContour',\n",
    "#                                                    'Utilities', 'LotConfig', 'LandSlope', 'Neighborhood',\n",
    "#                                                    'BldgType', 'HouseStyle'])\n",
    "# train_df1.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating a generic dictionary to hold numercial values to represent categorical values\n",
    "# for quality related columns (ExterQual, BsmtQual)\n",
    "# quality_ratings = {\n",
    "#  'NA':0,\n",
    "#  'Po':1, \n",
    "#  'Fa':2, \n",
    "#  'TA':3, \n",
    "#  'Gd':4, \n",
    "#  'Ex':5\n",
    "# }\n",
    "\n",
    "# I'm not sure about this trick! but I saw someone metioned the Mean Absolute Percentage Error (MAPE)\n",
    "# can be good accuracy for just not negative and non zero features!!\n",
    "# So, to stay in safe side I suggest we start from 1!\n",
    "quality_ratings = {\n",
    " 'NA':1,\n",
    " 'Po':2, \n",
    " 'Fa':3, \n",
    " 'TA':4, \n",
    " 'Gd':5, \n",
    " 'Ex':6\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting category labels to numerical values for ExterQual column\n",
    "train_df['ExterQual_Num'] = train_df.ExterQual.map(quality_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting category labels to numerical values for BsmtQual column\n",
    "train_df['BsmtQual_Num'] = train_df.BsmtQual.map(quality_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting category labels to numerical values for HeatingQC column\n",
    "train_df['HeatingQC_Num'] = train_df.HeatingQC.map(quality_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting category labels to numerical values for KitchenQual column\n",
    "train_df['KitchenQual_Num'] = train_df.KitchenQual.map(quality_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting category labels to numerical values for GarageQual column\n",
    "train_df['GarageQual_Num'] = train_df.GarageQual.map(quality_ratings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Converting category labels to numerical values for SaleCondition column\n",
    "# train_df['SaleCondition_Num'] = train_df.SaleCondition.map(\n",
    "# {'Abnorml':1, \n",
    "#  'AdjLand':2, \n",
    "#  'Alloca':3, \n",
    "#  'Family':4, \n",
    "#  'Normal':5,\n",
    "#  'Partial':6})\n",
    "\n",
    "# trying to make the weight meaningful based of some advise from a realstate agent freind!\n",
    "train_df['SaleCondition_Num'] = train_df.SaleCondition.map(\n",
    "{'Normal':6,\n",
    " 'Alloca':5,\n",
    " 'AdjLand':4,\n",
    " 'Family':3,\n",
    " 'Partial':2,\n",
    " 'Abnorml':1\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# drop the columns where majority of their values are missing\n",
    "train_df = train_df.drop(['ExterQual', 'BsmtQual', 'HeatingQC', 'KitchenQual','GarageQual','SaleCondition'], axis = 1)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create df that consists of columns and the number of missing values for each \n",
    "data = train_df.isnull().sum().sort_values(ascending=False)\n",
    "missing_df = pd.DataFrame(data=data, columns=['missing_cnt'])\n",
    "\n",
    "# add column and fill it with the percentage of those missing values\n",
    "missing_df['percent_missing'] = missing_df.missing_cnt.apply(lambda x : '{:.2f}'.format(x/train_df.shape[0] * 100)) \n",
    "missing_df = missing_df[missing_df.missing_cnt > 0]\n",
    "missing_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.update(train_df[['Electrical']].fillna('SBrkr'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#I'm not professional but I think there is no Masonry veneer type for the Vinyl and Cement!\n",
    "train_df.at[530,'MasVnrType']='Stone'\n",
    "\n",
    "# train_df['Set_of_Numbers'] = train_df['Set_of_Numbers'].fillna(0)\n",
    "train_df.update(train_df[['MasVnrArea']].fillna(0))\n",
    "train_df.update(train_df[['MasVnrType']].fillna('None'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Missing Data Imputation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The rest of missing data is numeric. So, I prefer to replace by mean of same column!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['LotFrontage'] = train_df['LotFrontage'].fillna((train_df['LotFrontage'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['BsmtQual_Num'] = train_df['BsmtQual_Num'].fillna((train_df['BsmtQual_Num'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df['GarageQual_Num'] = train_df['GarageQual_Num'].fillna((train_df['GarageQual_Num'].mean()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vida: let's speak about this part in our meeting! \n",
    "# Vida: I handled all null values one by one!\n",
    "\n",
    "# NA is a valid value and is some cases not equivalent to NaN and should not be converted to 0\n",
    "# replace null values in df w/ values that had most counts for each column\n",
    "train_df1 = train_df1.apply(lambda x: x.fillna(0) if x.dtype.kind in 'biufc' \\\n",
    "                          else x.fillna(train_df.columns.value_counts().idxmax()))\n",
    "test_df = test_df.apply(lambda x: x.fillna(0) if x.dtype.kind in 'biufc' \\\n",
    "                          else x.fillna(train_df.columns.value_counts().idxmax()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vida: I couldn't run this part!\n",
    "\n",
    "# encode object (categorical) columns in df\n",
    "enc_df = train_df1.select_dtypes(include=['object']).apply(LabelEncoder().fit_transform)\n",
    "\n",
    "# add encoded columns back into original train df\n",
    "train_df1[enc_df.columns] = enc_df\n",
    "\n",
    "train_df1.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Correlation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr_matrix = train_df.corr()\n",
    "corr_matrix['SalePrice'] = round(corr_matrix['SalePrice'],4)\n",
    "corr_matrix['SalePrice'].sort_values(ascending=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Variance Inflation Factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# not finished yet!!\n",
    "# X = add_constant(train_df)\n",
    "# pd.Series([variance_inflation_factor(X.values, i)\n",
    "#           for i in range(X.shape[1])], index=X.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check distribution of sales price\n",
    "train_df1.hist(column='SalePrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize sale price so that it can be evenly distributed\n",
    "train_df1['LogPrice'] = np.log(train_df1.SalePrice)\n",
    "train_df1.hist(column='LogPrice')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check for more skewed columns\n",
    "train_df1.skew().sort_values(ascending=False).head(30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# looking at correlation of numeric features to SalePrice column. \n",
    "# this will suggest which columns have a greater relationship with the SalePrice column \n",
    "corr = train_df1.corr().abs().unstack().sort_values(ascending=False)['LogPrice']\n",
    "corr = corr.iloc[1:]\n",
    "corr = pd.DataFrame(corr, columns = ['Correlation'])\n",
    "\n",
    "corr = corr[corr.Correlation > 0.50]\n",
    "corr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cols = []\n",
    "for ind in corr.index:\n",
    "    cols.append(ind)\n",
    "\n",
    "# create pair plot between columns that have correlation 50% and above\n",
    "sns.pairplot(train_df1[cols])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization (CAN BE DELETED):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Count the number of houses sold in a year\n",
    "#year_sold_pivot = train_df.pivot_table(index='YrSold', values='SalePrice', aggfunc='count')\n",
    "#print(year_sold_pivot)\n",
    "\n",
    "# Plotting the sum of sales per year\n",
    "sns.set_context('talk', font_scale=1) \n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='YrSold', y='SalePrice', data=train_df[['SalePrice', 'YrSold']], estimator=sum)\n",
    "plt.xlabel('Year Sold')\n",
    "plt.ylabel('Number of Sales')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the median sale price for each year\n",
    "\n",
    "#Checking the median price\n",
    "#year_sold_pivot = train_df.pivot_table(index='YrSold', values='SalePrice', aggfunc=np.median)\n",
    "#print(year_sold_pivot)\n",
    "\n",
    "sns.set_context('talk', font_scale=1) \n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='YrSold', y='SalePrice', data=train_df[['SalePrice', 'YrSold']], estimator=np.median)\n",
    "plt.xlabel('Year Sold')\n",
    "plt.ylabel('Median Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Displaying the median price for each year using box plot.  \n",
    "sns.set_context('talk', font_scale=1) \n",
    "plt.figure(figsize=(10,5))\n",
    "sns.boxplot(x='YrSold', y='SalePrice', data=train_df[['SalePrice', 'YrSold']])\n",
    "plt.xlabel('Year Sold')\n",
    "plt.ylabel('Median Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Displaying the median price based on overall quality of the house using box plot.\n",
    "sns.set_context('talk', font_scale=1) \n",
    "plt.figure(figsize=(12,10))\n",
    "sns.boxplot(x='OverallQual', y='SalePrice', data=train_df[['SalePrice', 'OverallQual']])\n",
    "plt.xlabel('Overall Quality')\n",
    "plt.ylabel('Median Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.set_context('talk', font_scale=1) \n",
    "sns.set_style('dark')\n",
    "plt.figure(figsize=(15,8))\n",
    "\n",
    "# Plot GarageArea vs sale price of house considering the number of cars that can\n",
    "# fit in the garage\n",
    "sns.scatterplot(x='GarageArea', y='SalePrice', hue=train_df.GarageCars.tolist(),\n",
    "            palette='Set2', data=train_df[['SalePrice', 'GarageArea','GarageCars']])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the median sale price based on external quality\n",
    "sns.set_context('talk', font_scale=1) \n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='ExterQual', y='SalePrice', data=train_df[['SalePrice', 'ExterQual']], estimator=np.median)\n",
    "plt.xlabel('External Quality')\n",
    "plt.ylabel('Median Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the median sale price based on basement quality\n",
    "sns.set_context('talk', font_scale=1) \n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='BsmtQual', y='SalePrice', data=train_df[['SalePrice', 'BsmtQual']], estimator=np.median)\n",
    "plt.xlabel('Basement Quality')\n",
    "plt.ylabel('Median Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the median sale price based on heating and air conditioning quality\n",
    "sns.set_context('talk', font_scale=1) \n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='HeatingQC', y='SalePrice', data=train_df[['SalePrice', 'HeatingQC']], estimator=np.median)\n",
    "plt.xlabel('Heating Quality')\n",
    "plt.ylabel('Median Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the median sale price based on kitchen quality\n",
    "sns.set_context('talk', font_scale=1) \n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='KitchenQual', y='SalePrice', data=train_df[['SalePrice', 'KitchenQual']], estimator=np.median)\n",
    "plt.xlabel('Kitchen Quality')\n",
    "plt.ylabel('Median Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the median sale price based on fireplace quality\n",
    "sns.set_context('talk', font_scale=1) \n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='FireplaceQu', y='SalePrice', data=train_df[['SalePrice', 'FireplaceQu']], estimator=np.median)\n",
    "plt.xlabel('Fireplace Quality')\n",
    "plt.ylabel('Median Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the median sale price based on garage quality\n",
    "sns.set_context('talk', font_scale=1) \n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='GarageQual', y='SalePrice', data=train_df[['SalePrice', 'GarageQual']], estimator=np.median)\n",
    "plt.xlabel('Garage Quality')\n",
    "plt.ylabel('Median Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plotting the median sale price based on pool quality\n",
    "sns.set_context('talk', font_scale=1) \n",
    "plt.figure(figsize=(10,5))\n",
    "sns.barplot(x='PoolQC', y='SalePrice', data=train_df[['SalePrice', 'PoolQC']], estimator=np.median)\n",
    "plt.xlabel('Pool Quality')\n",
    "plt.ylabel('Median Sale Price')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum of quality points.\n",
    "train_df['SumQuality'] = train_df.ExterQual_Num + train_df.BsmtQual_Num + train_df.HeatingQC_Num + train_df.KitchenQual_Num + train_df.GarageQual_Num + train_df.FireplaceQu_Num + train_df.GarageQual_Num + train_df.PoolQC_Num \n",
    "            \n",
    "#print(train_df['SumQuality'])\n",
    "\n",
    "# sum of quality points, removing less correlated features (< 0.5).\n",
    "train_df['SumMIQ'] = train_df.ExterQual_Num + train_df.BsmtQual_Num + train_df.KitchenQual_Num + train_df.FireplaceQu_Num + train_df.GarageQual_Num\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Garage area per car\n",
    "# May not need this one, the correlation to SalePrice is only slightly higher than GarageArea\n",
    "train_df['GarageAreaPerCar'] = train_df.GarageArea + train_df.GarageCars \n",
    "\n",
    "#print(train_df['GarageAreaPerCar'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = train_df.corr().abs().unstack().sort_values(ascending=False)['SalePrice']\n",
    "corr.head(40)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "################## DISREGARD LOGIC BELOW ######################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# encode object columns\n",
    "# enc_df = train_df.select_dtypes(include=['object']).apply(LabelEncoder().fit_transform)\n",
    "# enc_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# add encoded columns back into train df\n",
    "# train_df[enc_df.columns] = enc_df\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# normalize df except for price column\n",
    "# norm_df = (train_df - train_df.mean()) / (train_df.max() - train_df.min())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
